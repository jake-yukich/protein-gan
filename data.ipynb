{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Download PDB files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio.PDB import PDBList\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import contextlib\n",
    "\n",
    "def load_pdb_ids(file_path):\n",
    "    \"\"\"\n",
    "    Load PDB IDs from a text file.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the text file containing PDB IDs.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of PDB IDs.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        pdb_ids = file.read().splitlines()\n",
    "    return pdb_ids\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def filter_stdout(filter_words):\n",
    "    \"\"\"\n",
    "    Context manager to filter specific stdout messages.\n",
    "\n",
    "    Args:\n",
    "        filter_words (list): List of words to filter out from stdout.\n",
    "\n",
    "    Yields:\n",
    "        None\n",
    "    \"\"\"\n",
    "    class FilteredStream:\n",
    "        def __init__(self, stream):\n",
    "            self.stream = stream\n",
    "\n",
    "        def write(self, message):\n",
    "            if not any(word in message for word in filter_words):\n",
    "                self.stream.write(message)\n",
    "\n",
    "        def flush(self):\n",
    "            self.stream.flush()\n",
    "\n",
    "    old_stdout = sys.stdout\n",
    "    sys.stdout = FilteredStream(sys.stdout)\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        sys.stdout = old_stdout\n",
    "\n",
    "def download_pdb_file(pdb_id, save_dir):\n",
    "    \"\"\"\n",
    "    Download a single PDB file.\n",
    "\n",
    "    Args:\n",
    "        pdb_id (str): The PDB ID of the file to download.\n",
    "        save_dir (str): The directory to save the downloaded PDB file.\n",
    "\n",
    "    Returns:\n",
    "        str: A message indicating the result of the download attempt.\n",
    "\n",
    "    Raises:\n",
    "        Exception: If there is an error during the download.\n",
    "    \"\"\"\n",
    "    pdbl = PDBList()  # using Biopython's PDBList class\n",
    "    file_path = os.path.join(save_dir, f\"pdb{pdb_id}.ent\")\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        try:\n",
    "            with filter_stdout([\"Downloading PDB structure\", \"Desired structure doesn't exist\"]):\n",
    "                pdbl.retrieve_pdb_file(pdb_id, pdir=save_dir, file_format='pdb', overwrite=False)\n",
    "            return f\"Downloaded {pdb_id}\"\n",
    "        except Exception as e:\n",
    "            return f\"Error downloading {pdb_id}: {str(e)}\"\n",
    "    else:\n",
    "        return f\"Skipped {pdb_id}, already exists\"\n",
    "\n",
    "def download_pdb_files(pdb_ids, save_dir='pdb_files', num_threads=16):\n",
    "    \"\"\"\n",
    "    Download PDB files using multiple threads and a progress bar.\n",
    "\n",
    "    Args:\n",
    "        pdb_ids (list): List of PDB IDs to download.\n",
    "        save_dir (str): The directory to save the downloaded PDB files.\n",
    "        num_threads (int): The number of threads to use for downloading.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    \n",
    "    results = []\n",
    "    with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "        futures = {executor.submit(download_pdb_file, pdb_id, save_dir): pdb_id for pdb_id in pdb_ids}\n",
    "        for future in tqdm(as_completed(futures), total=len(futures), desc=\"Downloading PDB files\", unit=\"file\", leave=True):\n",
    "            results.append(future.result())\n",
    "    \n",
    "    for result in results:\n",
    "        print(result)\n",
    "\n",
    "# Load PDB IDs from supplement-provided text files\n",
    "train_pdb_ids = load_pdb_ids('train_ids.txt')\n",
    "test_pdb_ids = load_pdb_ids('test_ids.txt')\n",
    "\n",
    "# Download PDB files\n",
    "download_pdb_files(train_pdb_ids, save_dir='pdb_files/train')\n",
    "download_pdb_files(test_pdb_ids, save_dir='pdb_files/test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Get pairwise distance matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"To create our datasets,\n",
    "# we extract non-overlapping fragments of lengths 16, 64, and 128 from chain ‘A’ for each protein\n",
    "# structure starting at the first residue and calculate the pairwise distance matrices from the alpha-carbon\n",
    "# coordinate positions\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from Bio import PDB\n",
    "from tqdm import tqdm\n",
    "\n",
    "def load_structure(pdb_file):\n",
    "    \"\"\"Load a PDB structure and return the first model and chain A.\"\"\"\n",
    "    parser = PDB.PDBParser(QUIET=True)\n",
    "    structure = parser.get_structure(\"protein\", pdb_file)\n",
    "    model = structure[0]\n",
    "    chain_a = model[\"A\"]\n",
    "    return chain_a\n",
    "\n",
    "def extract_fragments(chain, fragment_length):\n",
    "    \"\"\"Extract non-overlapping fragments of specified length from the chain.\"\"\"\n",
    "    fragments = []\n",
    "    residues = list(chain.get_residues())\n",
    "    \n",
    "    for i in range(0, len(residues) - fragment_length + 1, fragment_length):\n",
    "        fragment = residues[i:i+fragment_length]\n",
    "        if len(fragment) == fragment_length:\n",
    "            fragments.append(fragment)\n",
    "    \n",
    "    return fragments\n",
    "\n",
    "def calculate_distance_matrix(fragment):\n",
    "    \"\"\"Calculate pairwise distance matrix for alpha carbons in the fragment.\"\"\"\n",
    "    coords = []\n",
    "    for residue in fragment:\n",
    "        if \"CA\" in residue:\n",
    "            coords.append(residue[\"CA\"].coord)\n",
    "    \n",
    "    coords = np.array(coords)\n",
    "    dist_matrix = np.linalg.norm(coords[:, np.newaxis] - coords, axis=2)\n",
    "    return dist_matrix\n",
    "\n",
    "def process_pdb_file(pdb_file, fragment_lengths=[16, 64, 128]):\n",
    "    \"\"\"Process a single PDB file and return distance matrices for each fragment length.\"\"\"\n",
    "    chain = load_structure(pdb_file)\n",
    "    results = {length: [] for length in fragment_lengths}\n",
    "    \n",
    "    for length in fragment_lengths:\n",
    "        fragments = extract_fragments(chain, length)\n",
    "        for fragment in fragments:\n",
    "            dist_matrix = calculate_distance_matrix(fragment)\n",
    "            if dist_matrix.shape[0] == length:\n",
    "                results[length].append(dist_matrix)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def create_datasets(pdb_dir, output_dir, fragment_lengths=[16, 64, 128]):\n",
    "    \"\"\"Create datasets from PDB files in the specified directory.\"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    pdb_files = [f for f in os.listdir(pdb_dir) if f.endswith('.ent') or f.endswith('.pdb')]\n",
    "    \n",
    "    for length in fragment_lengths:\n",
    "        all_matrices = []\n",
    "        \n",
    "        for pdb_file in tqdm(pdb_files, desc=f\"Processing {length}-residue fragments\"):\n",
    "            results = process_pdb_file(os.path.join(pdb_dir, pdb_file), [length])\n",
    "            all_matrices.extend(results[length])\n",
    "        \n",
    "        dataset = np.array(all_matrices)\n",
    "        np.save(os.path.join(output_dir, f'distance_matrices_{length}.npy'), dataset)\n",
    "        print(f\"Saved {len(dataset)} matrices of size {length}x{length}\")\n",
    "\n",
    "create_datasets('pdb_files/train', 'datasets/train')\n",
    "create_datasets('pdb_files/test', 'datasets/test')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".protganenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
