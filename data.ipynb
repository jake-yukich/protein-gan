{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Download PDB files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio.PDB import PDBList\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import contextlib\n",
    "\n",
    "def load_pdb_ids(file_path):\n",
    "    \"\"\"\n",
    "    Load PDB IDs from a text file.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the text file containing PDB IDs.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of PDB IDs.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        pdb_ids = file.read().splitlines()\n",
    "    return pdb_ids\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def filter_stdout(filter_words):\n",
    "    \"\"\"\n",
    "    Context manager to filter specific stdout messages.\n",
    "\n",
    "    Args:\n",
    "        filter_words (list): List of words to filter out from stdout.\n",
    "\n",
    "    Yields:\n",
    "        None\n",
    "    \"\"\"\n",
    "    class FilteredStream:\n",
    "        def __init__(self, stream):\n",
    "            self.stream = stream\n",
    "\n",
    "        def write(self, message):\n",
    "            if not any(word in message for word in filter_words):\n",
    "                self.stream.write(message)\n",
    "\n",
    "        def flush(self):\n",
    "            self.stream.flush()\n",
    "\n",
    "    old_stdout = sys.stdout\n",
    "    sys.stdout = FilteredStream(sys.stdout)\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        sys.stdout = old_stdout\n",
    "\n",
    "def download_pdb_file(pdb_id, save_dir):\n",
    "    \"\"\"\n",
    "    Download a single PDB file.\n",
    "\n",
    "    Args:\n",
    "        pdb_id (str): The PDB ID of the file to download.\n",
    "        save_dir (str): The directory to save the downloaded PDB file.\n",
    "\n",
    "    Returns:\n",
    "        str: A message indicating the result of the download attempt.\n",
    "\n",
    "    Raises:\n",
    "        Exception: If there is an error during the download.\n",
    "    \"\"\"\n",
    "    pdbl = PDBList()  # using Biopython's PDBList class\n",
    "    file_path = os.path.join(save_dir, f\"pdb{pdb_id}.ent\")\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        try:\n",
    "            with filter_stdout([\"Downloading PDB structure\", \"Desired structure doesn't exist\"]):\n",
    "                pdbl.retrieve_pdb_file(pdb_id, pdir=save_dir, file_format='pdb', overwrite=False)\n",
    "            return f\"Downloaded {pdb_id}\"\n",
    "        except Exception as e:\n",
    "            return f\"Error downloading {pdb_id}: {str(e)}\"\n",
    "    else:\n",
    "        return f\"Skipped {pdb_id}, already exists\"\n",
    "\n",
    "def download_pdb_files(pdb_ids, save_dir='pdb_files', num_threads=16):\n",
    "    \"\"\"\n",
    "    Download PDB files using multiple threads and a progress bar.\n",
    "\n",
    "    Args:\n",
    "        pdb_ids (list): List of PDB IDs to download.\n",
    "        save_dir (str): The directory to save the downloaded PDB files.\n",
    "        num_threads (int): The number of threads to use for downloading.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    \n",
    "    results = []\n",
    "    with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "        futures = {executor.submit(download_pdb_file, pdb_id, save_dir): pdb_id for pdb_id in pdb_ids}\n",
    "        for future in tqdm(as_completed(futures), total=len(futures), desc=\"Downloading PDB files\", unit=\"file\", leave=True):\n",
    "            results.append(future.result())\n",
    "    \n",
    "    for result in results:\n",
    "        print(result)\n",
    "\n",
    "# Load PDB IDs from supplement-provided text files\n",
    "train_pdb_ids = load_pdb_ids('train_ids.txt')\n",
    "test_pdb_ids = load_pdb_ids('test_ids.txt')\n",
    "\n",
    "# Download PDB files\n",
    "download_pdb_files(train_pdb_ids, save_dir='pdb_files/train')\n",
    "download_pdb_files(test_pdb_ids, save_dir='pdb_files/test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Get pairwise distance matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 16-residue fragments: 100%|██████████| 115535/115535 [2:16:11<00:00, 14.14it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 1927226 matrices of size 16x16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 16-residue fragments: 100%|██████████| 6234/6234 [10:05<00:00, 10.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 75274 matrices of size 16x16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# \"To create our datasets,\n",
    "#  we extract non-overlapping fragments of lengths 16, 64, and 128 from chain ‘A’ for each protein\n",
    "#  structure starting at the first residue and calculate the pairwise distance matrices from the alpha-carbon\n",
    "#  coordinate positions\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from Bio import PDB\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def load_structure(pdb_file):\n",
    "    parser = PDB.PDBParser(QUIET=True)\n",
    "    structure = parser.get_structure(\"protein\", pdb_file)\n",
    "    return structure[0][\"A\"]  # Return the first model and chain A\n",
    "\n",
    "def extract_fragments(chain, fragment_length):\n",
    "    fragments = []\n",
    "    residues = list(chain.get_residues())\n",
    "    for i in range(0, len(residues) - fragment_length + 1, fragment_length):\n",
    "        fragment = residues[i:i+fragment_length]\n",
    "        if len(fragment) == fragment_length:\n",
    "            fragments.append(fragment)\n",
    "    return fragments\n",
    "\n",
    "def calculate_distance_matrix(fragment):\n",
    "    coords = []\n",
    "    for residue in fragment:\n",
    "        if \"CA\" in residue:\n",
    "            coords.append(residue[\"CA\"].coord)\n",
    "    coords = np.array(coords)\n",
    "    if len(coords) < 2:\n",
    "        return np.array([])\n",
    "    diff = coords[:, np.newaxis, :] - coords[np.newaxis, :, :]\n",
    "    dist_matrix = np.sqrt(np.sum(diff**2, axis=-1))\n",
    "    return dist_matrix\n",
    "\n",
    "def process_pdb_file(pdb_file, fragment_length):\n",
    "    try:\n",
    "        chain = load_structure(pdb_file)\n",
    "        fragments = extract_fragments(chain, fragment_length)\n",
    "        matrices = []\n",
    "        for fragment in fragments:\n",
    "            dist_matrix = calculate_distance_matrix(fragment)\n",
    "            if dist_matrix.shape == (fragment_length, fragment_length):\n",
    "                matrices.append(dist_matrix)\n",
    "        return matrices\n",
    "    except Exception as e:\n",
    "        return []\n",
    "\n",
    "def create_datasets(pdb_dir, output_dir, fragment_lengths=[16, 64, 128], num_threads=16, batch_size=1000):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    pdb_files = [f for f in os.listdir(pdb_dir) if f.endswith('.ent') or f.endswith('.pdb')]\n",
    "    \n",
    "    for length in fragment_lengths:\n",
    "        output_file = os.path.join(output_dir, f'distance_matrices_{length}.npy')\n",
    "        \n",
    "        if os.path.exists(output_file):\n",
    "            print(f\"Skipping {length}-residue fragments, output file already exists\")\n",
    "            continue\n",
    "        \n",
    "        batch_matrices = []\n",
    "        total_matrices = 0\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "            futures = {executor.submit(process_pdb_file, os.path.join(pdb_dir, f), length): f for f in pdb_files}\n",
    "\n",
    "            for future in tqdm(as_completed(futures), total=len(futures), desc=f\"Processing {length}-residue fragments\"):\n",
    "                matrices = future.result()\n",
    "                batch_matrices.extend(matrices)\n",
    "                \n",
    "                if len(batch_matrices) >= batch_size:\n",
    "                    # Save batch to disk\n",
    "                    batch_array = np.array(batch_matrices)\n",
    "                    if total_matrices == 0:\n",
    "                        np.save(output_file, batch_array)\n",
    "                    else:\n",
    "                        with open(output_file, 'ab') as f:\n",
    "                            np.save(f, batch_array)\n",
    "                    total_matrices += len(batch_matrices)\n",
    "                    batch_matrices = []\n",
    "        \n",
    "        if batch_matrices:\n",
    "            batch_array = np.array(batch_matrices)\n",
    "            if total_matrices == 0:\n",
    "                np.save(output_file, batch_array)\n",
    "            else:\n",
    "                with open(output_file, 'ab') as f:\n",
    "                    np.save(f, batch_array)\n",
    "            total_matrices += len(batch_matrices)\n",
    "        \n",
    "        print(f\"Saved {total_matrices} matrices of size {length}x{length}\")\n",
    "\n",
    "create_datasets('pdb_files/train', 'datasets/train', [16])\n",
    "create_datasets('pdb_files/test', 'datasets/test', [16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the .npy files created in the previous cell contain multiple arrays (one per batch)\n",
    "# to concatenate, use something like the following:\n",
    "\n",
    "# def load_batched_npy(file_path):\n",
    "#     with open(file_path, 'rb') as f:\n",
    "#         arrays = []\n",
    "#         while True:\n",
    "#             try:\n",
    "#                 arrays.append(np.load(f))\n",
    "#             except ValueError:\n",
    "#                 break\n",
    "#     return np.concatenate(arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".protganenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
